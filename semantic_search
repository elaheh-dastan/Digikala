# Semantic Search
It is not an explanation of the project. just a few points I may forget by time.

## Source model
Our teacher model distilled to a student 6 layer Bert model is BGE-m3 which is then fine tuned with our own custom dataset. For fine tune training we use CLIP approach and 
cross entropy loss function.

## InfoNCE vs Cross Entropy
Mathematically → the same thing.

Conceptually → InfoNCE is cross-entropy loss used in a contrastive learning context.

### Small but Important Tweaks
a) Temperature Scaling (τ)

InfoNCE always includes dividing logits by a temperature, which sharpens or softens the softmax distribution.
Plain cross-entropy doesn’t necessarily have that.

If you divide them by a small τ, say τ = 0.05, you make the differences larger before softmax. So now, the model becomes very confident that the positive is correct — 
it pushes embeddings apart more aggressively.

If you use a large τ, say τ = 1.0, everything becomes softer and the model is less confident. That makes the loss smaller, but the model learns less discriminative representations.

b) Batch Negatives

In InfoNCE, all other items in the batch act as negatives — this is not part of the standard cross-entropy formulation.

c) Symmetry

InfoNCE is often applied in both directions (query→doc and doc→query), while normal CE is one-way.

d) Typical implementation

Cross Entropy: F.cross_entropy()
InfoNCE: F.cross_entropy(sim / τ, labels)

## indexing
FAISS and HNSW are in the same family of tools


## Batch size
Contrastive learning depends heavily on batch size, the bigger the better since it is more probable for the model to see better negatives not just random far ones

## Cosine Similarity vs. Dot Product
Dot product measures absolute similarity (large magnitudes ⇒ large scores).

Cosine similarity measures directional similarity (angle between vectors).

Dot Product is used when vector magnitude encodes confidence or importance.

## LR importance
Contrastive learning fine-tunes the angles between embeddings, not their magnitudes.
Because of this geometric sensitivity, the learning rate must be much smaller and smoother than in typical classification training — otherwise, you destroy 
the pretrained semantic structure instead of refining it.

## Hard negatives
Our harware resources may not be enough for very large batch sizes but we have to make sure our model sees hard negatives like mobile vs mobile cover
